/*******************************************************************************
 * Copyright (c) 2015, 2017 IBM Corp. and others
 *
 * This program and the accompanying materials are made available under
 * the terms of the Eclipse Public License 2.0 which accompanies this
 * distribution and is available at https://www.eclipse.org/legal/epl-2.0/
 * or the Apache License, Version 2.0 which accompanies this distribution and
 * is available at https://www.apache.org/licenses/LICENSE-2.0.
 *
 * This Source Code may also be made available under the following
 * Secondary Licenses when the conditions for such availability set
 * forth in the Eclipse Public License, v. 2.0 are satisfied: GNU
 * General Public License, version 2 with the GNU Classpath
 * Exception [1] and GNU General Public License, version 2 with the
 * OpenJDK Assembly Exception [2].
 *
 * [1] https://www.gnu.org/software/classpath/license.html
 * [2] http://openjdk.java.net/legal/assembly-exception.html
 *
 * SPDX-License-Identifier: EPL-2.0 OR Apache-2.0 OR GPL-2.0 WITH Classpath-exception-2.0 OR LicenseRef-GPL-2.0 WITH Assembly-exception
 *******************************************************************************/

/*
 * Calculate the checksum of data that is 16 byte aligned and a multiple of
 * 16 bytes.
 *
 * The first step is to reduce it to 1024 bits. We do this in 8 parallel
 * chunks in order to mask the latency of the vpmsum instructions. If we
 * have more than 32 kB of data to checksum we repeat this step multiple
 * times, passing in the previous 1024 bits.
 *
 * The next step is to reduce the 1024 bits to 64 bits. This step adds
 * 32 bits of 0s to the end - this matches what a CRC does. We just
 * calculate constants that land the data in this 32 bits.
 *
 * We then use fixed point Barrett reduction to compute a mod n over GF(2)
 * for n = CRC using POWER8 instructions. We use x = 32.
 *
 * http://en.wikipedia.org/wiki/Barrett_reduction
 *
 */

#include "j9cfg.h"
#include "jilconsts.inc"
#include "p/runtime/ppcasmdefines.inc"

#undef toc

#ifndef r1
#define r1 1
#endif

#ifndef r2
#define r2 2
#endif

#if defined(AIXPPC)
        .toc
T.byteswap_constant:
        .tc     .byteswap_constant[TC],.byteswap_constant
T.constants:
        .tc     .constants[TC],.constants
T.barrett_constants:
        .tc     .barrett_constants[TC],.barrett_constants
T.short_constants:
        .tc     .short_constants[TC],.short_constants

        .csect  PPCcrc32_DATA{RO},4
.byteswap_constant:
        /* byte reverse permute constant */
        .llong 0x0F0E0D0C0B0A0908
        .llong 0x0706050403020100
#elif defined(LINUXPPC64) || defined(LINUXPPC)
#if defined(LINUXPPC64)
#if !defined(__LITTLE_ENDIAN__)
       .section  ".toc"
       .section  ".opd","aw"
       .align    3
       .globl    __crc32_vpmsum  
       .size     __crc32_vpmsum,24
__crc32_vpmsum:
       .quad     .__crc32_vpmsum
       .quad     .TOC.@tocbase
       .long     0x00000000
       .long     0x00000000
#endif 

       .section .rodata
#else
       .rodata
#endif
.balign 16

.byteswap_constant:
        /* byte reverse permute constant */
        .octa 0x0F0E0D0C0B0A09080706050403020100
#endif

#define __ASSEMBLY__
#include "p/runtime/J9PPCCRC32_constants.h"

#if defined(AIXPPC)
        .csect       PPCcrc32_TEXT{PR}
        .machine     "pwr7"
#elif defined(LINUXPPC64)
        .section     .text
#else
        .text
#endif

#if defined(__BIG_ENDIAN__) && defined(REFLECT)
#define BYTESWAP_DATA
#elif defined(__LITTLE_ENDIAN__) && !defined(REFLECT)
#define BYTESWAP_DATA
#else
#undef BYTESWAP_DATA
#endif

#define off16       r25
#define off32       r26
#define off48       r27
#define off64       r28
#define off80       r29
#define off96       r30
#define off112      r31

#define const1      vr24
#define const2      vr25

#define byteswap    vr26
#define mask_32bit  vr27
#define mask_64bit  vr28
#define zeroes      vr29

#ifdef BYTESWAP_DATA
#define VPERM(A, B, C, D) vperm A, B, C, D
#else
#define VPERM(A, B, C, D)
#endif

#if defined(AIXPPC)
        .globl    .__crc32_vpmsum
#elif defined(LINUXPPC64)
        .globl    FUNC_LABEL(__crc32_vpmsum)
        .type     FUNC_LABEL(__crc32_vpmsum),@function
#else
        .globl    __crc32_vpmsum
#endif

/* unsigned int __crc32_vpmsum(unsigned int crc, void *p, unsigned long len) */
#if defined(AIXPPC)
.__crc32_vpmsum:
   .function .__crc32_vpmsum,startproc.__crc32_vpmsum,16,0,(endproc.__crc32_vpmsum-startproc.__crc32_vpmsum)
#elif defined(LINUXPPC64)
FUNC_LABEL(__crc32_vpmsum):
#else
__crc32_vpmsum:
#endif

startproc.__crc32_vpmsum:
        staddr  r31,-8(r1)
        staddr  r30,-16(r1)
        staddr  r29,-24(r1)
        staddr  r28,-32(r1)
        staddr  r27,-40(r1)
        staddr  r26,-48(r1)
        staddr  r25,-56(r1)

        li      off16,16
        li      off32,32
        li      off48,48
        li      off64,64
        li      off80,80
        li      off96,96
        li      off112,112
        li      r0,0

        /* Enough room for saving 10 non volatile VMX registers */
        subi    r6,r1,64+10*16
        subi    r7,r1,64+2*16

        stvx    vr20,0,r6
        stvx    vr21,off16,r6
        stvx    vr22,off32,r6
        stvx    vr23,off48,r6
        stvx    vr24,off64,r6
        stvx    vr25,off80,r6
        stvx    vr26,off96,r6
        stvx    vr27,off112,r6
        stvx    vr28,0,r7
        stvx    vr29,off16,r7

        mr      r10,r3

        vxor    zeroes,zeroes,zeroes
        vspltisw vr0,-1

        vsldoi  mask_32bit,zeroes,vr0,4
        vsldoi  mask_64bit,zeroes,vr0,8

        /* Get the initial value into vr8 */
        vxor    vr8,vr8,vr8
#if defined(TR_TARGET_64BIT)
        MTVSRD(vs40, r3)
#else
        MTVSRWZ(vs40, r3)
#endif
#ifdef REFLECT
        vsldoi  vr8,zeroes,vr8,8        /* shift into bottom 32 bits */
#else
        vsldoi  vr8,vr8,zeroes,4        /* shift into top 32 bits */
#endif

#ifdef BYTESWAP_DATA
#if defined(AIXPPC)
        laddr   r3,T.byteswap_constant(r2)
#elif defined(LINUXPPC64)
        addis   r3,r2,.byteswap_constant@toc@ha
        addi    r3,r3,.byteswap_constant@toc@l
#else
        lis     r3,.byteswap_constant@ha
        addi    r3,r3,.byteswap_constant@l
#endif

        lvx     byteswap,0,r3
        addi    r3,r3,16
#endif

        cmpi    cr0,CmpAddr,r5,256
        blt     .Lshort

#if defined(TR_TARGET_64BIT)
        rldicr  r6,r5,0,56
#else
        rlwinm  r6,r5,0,0,24
#endif

        /* Checksum in blocks of MAX_SIZE */
#if defined(AIXPPC)
.Ll1:   lis     r7,(MAX_SIZE>16) & 0x0000FFFF
#elif defined(LINUXPPC64) || defined(LINUXPPC)
.Ll1:   lis     r7,(MAX_SIZE>>16) & 0x0000FFFF
#endif
        ori     r7,r7,MAX_SIZE & 0x0000FFFF
        mr      r9,r7
        cmp     cr0,CmpAddr,r6,r7
        bgt     .Ll2
        mr      r7,r6
.Ll2:   subf    r6,r7,r6

        /* our main loop does 128 bytes at a time */
        ShiftRI r7,r7,7

        /*
         * Work out the offset into the constants table to start at. Each
         * constant is 16 bytes, and it is used against 128 bytes of input
         * data - 128 / 16 = 8
         */
        ShiftLI r8,r7,4
        ShiftRI r9,r9,3
        subf    r8,r8,r9

        /* We reduce our final 128 bytes in a separate step */
        addi    r7,r7,-1
        mtctr   r7

#if defined(AIXPPC)
        laddr   r3,T.constants(r2)
#elif defined(LINUXPPC64)
        addis   r3,r2,.constants@toc@ha
        addi    r3,r3,.constants@toc@l
#else
        lis     r3,.constants@ha
        addi    r3,r3,.constants@l
#endif

        /* Find the start of our constants */
        add     r3,r3,r8

        /* zero vr0-vr7 which will contain our checksums */
        vxor    vr0,vr0,vr0
        vxor    vr1,vr1,vr1
        vxor    vr2,vr2,vr2
        vxor    vr3,vr3,vr3
        vxor    vr4,vr4,vr4
        vxor    vr5,vr5,vr5
        vxor    vr6,vr6,vr6
        vxor    vr7,vr7,vr7

        lvx     const1,0,r3

        /*
         * If we are looping back to consume more data we use the values
         * already in vr16-vr23.
         */
        cmpi    cr0,CmpAddr,r0,1
        beq     .Ll3

        /* First warm up pass */
        lvx     vr16,0,r4
        lvx     vr17,off16,r4
        VPERM(vr16,vr16,vr16,byteswap)
        VPERM(vr17,vr17,vr17,byteswap)
        lvx     vr18,off32,r4
        lvx     vr19,off48,r4
        VPERM(vr18,vr18,vr18,byteswap)
        VPERM(vr19,vr19,vr19,byteswap)
        lvx     vr20,off64,r4
        lvx     vr21,off80,r4
        VPERM(vr20,vr20,vr20,byteswap)
        VPERM(vr21,vr21,vr21,byteswap)
        lvx     vr22,off96,r4
        lvx     vr23,off112,r4
        VPERM(vr22,vr22,vr22,byteswap)
        VPERM(vr23,vr23,vr23,byteswap)
        addi    r4,r4,8*16

        /* xor in initial value */
        vxor    vr16,vr16,vr8

.Ll3:   bdz     .Lfirst_warm_up_done

        addi    r3,r3,16
        lvx     const2,0,r3

        /* Second warm up pass */
        VPMSUMD(vr8,vr16,const1)
        lvx     vr16,0,r4
        VPERM(vr16,vr16,vr16,byteswap)
        ori     r2,r2,0

        VPMSUMD(vr9,vr17,const1)
        lvx     vr17,off16,r4
        VPERM(vr17,vr17,vr17,byteswap)
        ori     r2,r2,0

        VPMSUMD(vr10,vr18,const1)
        lvx     vr18,off32,r4
        VPERM(vr18,vr18,vr18,byteswap)
        ori     r2,r2,0

        VPMSUMD(vr11,vr19,const1)
        lvx     vr19,off48,r4
        VPERM(vr19,vr19,vr19,byteswap)
        ori     r2,r2,0

        VPMSUMD(vr12,vr20,const1)
        lvx     vr20,off64,r4
        VPERM(vr20,vr20,vr20,byteswap)
        ori     r2,r2,0

        VPMSUMD(vr13,vr21,const1)
        lvx     vr21,off80,r4
        VPERM(vr21,vr21,vr21,byteswap)
        ori     r2,r2,0

        VPMSUMD(vr14,vr22,const1)
        lvx     vr22,off96,r4
        VPERM(vr22,vr22,vr22,byteswap)
        ori     r2,r2,0

        VPMSUMD(vr15,vr23,const1)
        lvx     vr23,off112,r4
        VPERM(vr23,vr23,vr23,byteswap)

        addi    r4,r4,8*16

        bdz     .Lfirst_cool_down

        /*
         * main loop. We modulo schedule it such that it takes three iterations
         * to complete - first iteration load, second iteration vpmsum, third
         * iteration xor.
         */
#if defined(AIXPPC)
        .align  4
#elif defined(LINUXPPC64) || defined(LINUXPPC)
        .balign 16
#endif
.Ll4:   lvx     const1,0,r3
        addi    r3,r3,16
        ori     r2,r2,0

        vxor    vr0,vr0,vr8
        VPMSUMD(vr8,vr16,const2)
        lvx     vr16,0,r4
        VPERM(vr16,vr16,vr16,byteswap)
        ori     r2,r2,0

        vxor    vr1,vr1,vr9
        VPMSUMD(vr9,vr17,const2)
        lvx     vr17,off16,r4
        VPERM(vr17,vr17,vr17,byteswap)
        ori     r2,r2,0

        vxor    vr2,vr2,vr10
        VPMSUMD(vr10,vr18,const2)
        lvx     vr18,off32,r4
        VPERM(vr18,vr18,vr18,byteswap)
        ori     r2,r2,0

        vxor    vr3,vr3,vr11
        VPMSUMD(vr11,vr19,const2)
        lvx     vr19,off48,r4
        VPERM(vr19,vr19,vr19,byteswap)
        lvx     const2,0,r3
        ori     r2,r2,0

        vxor    vr4,vr4,vr12
        VPMSUMD(vr12,vr20,const1)
        lvx     vr20,off64,r4
        VPERM(vr20,vr20,vr20,byteswap)
        ori     r2,r2,0

        vxor    vr5,vr5,vr13
        VPMSUMD(vr13,vr21,const1)
        lvx     vr21,off80,r4
        VPERM(vr21,vr21,vr21,byteswap)
        ori     r2,r2,0

        vxor    vr6,vr6,vr14
        VPMSUMD(vr14,vr22,const1)
        lvx     vr22,off96,r4
        VPERM(vr22,vr22,vr22,byteswap)
        ori     r2,r2,0

        vxor    vr7,vr7,vr15
        VPMSUMD(vr15,vr23,const1)
        lvx     vr23,off112,r4
        VPERM(vr23,vr23,vr23,byteswap)

        addi    r4,r4,8*16

        bdnz    .Ll4

.Lfirst_cool_down:
        /* First cool down pass */
        lvx     const1,0,r3
        addi    r3,r3,16

        vxor    vr0,vr0,vr8
        VPMSUMD(vr8,vr16,const1)
        ori     r2,r2,0

        vxor    vr1,vr1,vr9
        VPMSUMD(vr9,vr17,const1)
        ori     r2,r2,0

        vxor    vr2,vr2,vr10
        VPMSUMD(vr10,vr18,const1)
        ori     r2,r2,0

        vxor    vr3,vr3,vr11
        VPMSUMD(vr11,vr19,const1)
        ori     r2,r2,0

        vxor    vr4,vr4,vr12
        VPMSUMD(vr12,vr20,const1)
        ori     r2,r2,0

        vxor    vr5,vr5,vr13
        VPMSUMD(vr13,vr21,const1)
        ori     r2,r2,0

        vxor    vr6,vr6,vr14
        VPMSUMD(vr14,vr22,const1)
        ori     r2,r2,0

        vxor    vr7,vr7,vr15
        VPMSUMD(vr15,vr23,const1)
        ori     r2,r2,0

.Lsecond_cool_down:
        /* Second cool down pass */
        vxor    vr0,vr0,vr8
        vxor    vr1,vr1,vr9
        vxor    vr2,vr2,vr10
        vxor    vr3,vr3,vr11
        vxor    vr4,vr4,vr12
        vxor    vr5,vr5,vr13
        vxor    vr6,vr6,vr14
        vxor    vr7,vr7,vr15

#ifdef REFLECT
        /*
         * vpmsumd produces a 96 bit result in the least significant bits
         * of the register. Since we are bit reflected we have to shift it
         * left 32 bits so it occupies the least significant bits in the
         * bit reflected domain.
         */
        vsldoi  vr0,vr0,zeroes,4
        vsldoi  vr1,vr1,zeroes,4
        vsldoi  vr2,vr2,zeroes,4
        vsldoi  vr3,vr3,zeroes,4
        vsldoi  vr4,vr4,zeroes,4
        vsldoi  vr5,vr5,zeroes,4
        vsldoi  vr6,vr6,zeroes,4
        vsldoi  vr7,vr7,zeroes,4
#endif

        /* xor with last 1024 bits */
        lvx     vr8,0,r4
        lvx     vr9,off16,r4
        VPERM(vr8,vr8,vr8,byteswap)
        VPERM(vr9,vr9,vr9,byteswap)
        lvx     vr10,off32,r4
        lvx     vr11,off48,r4
        VPERM(vr10,vr10,vr10,byteswap)
        VPERM(vr11,vr11,vr11,byteswap)
        lvx     vr12,off64,r4
        lvx     vr13,off80,r4
        VPERM(vr12,vr12,vr12,byteswap)
        VPERM(vr13,vr13,vr13,byteswap)
        lvx     vr14,off96,r4
        lvx     vr15,off112,r4
        VPERM(vr14,vr14,vr14,byteswap)
        VPERM(vr15,vr15,vr15,byteswap)

        addi    r4,r4,8*16

        vxor    vr16,vr0,vr8
        vxor    vr17,vr1,vr9
        vxor    vr18,vr2,vr10
        vxor    vr19,vr3,vr11
        vxor    vr20,vr4,vr12
        vxor    vr21,vr5,vr13
        vxor    vr22,vr6,vr14
        vxor    vr23,vr7,vr15

        li      r0,1
        cmpi    cr0,CmpAddr,r6,0
        addi    r6,r6,128
        bne     .Ll1

        /* Work out how many bytes we have left */
        andi.   r5,r5,127

        /* Calculate where in the constant table we need to start */
        subfic  r6,r5,128
        add     r3,r3,r6

        /* How many 16 byte chunks are in the tail */
        ShiftRI r7,r5,4
        mtctr   r7

        /*
         * Reduce the previously calculated 1024 bits to 64 bits, shifting
         * 32 bits to include the trailing 32 bits of zeros
         */
        lvx     vr0,0,r3
        lvx     vr1,off16,r3
        lvx     vr2,off32,r3
        lvx     vr3,off48,r3
        lvx     vr4,off64,r3
        lvx     vr5,off80,r3
        lvx     vr6,off96,r3
        lvx     vr7,off112,r3
        addi    r3,r3,8*16

        VPMSUMW(vr0,vr16,vr0)
        VPMSUMW(vr1,vr17,vr1)
        VPMSUMW(vr2,vr18,vr2)
        VPMSUMW(vr3,vr19,vr3)
        VPMSUMW(vr4,vr20,vr4)
        VPMSUMW(vr5,vr21,vr5)
        VPMSUMW(vr6,vr22,vr6)
        VPMSUMW(vr7,vr23,vr7)

        /* Now reduce the tail (0 - 112 bytes) */
        cmpi    cr0,CmpAddr,r7,0
        beq     .Ll5

        lvx     vr16,0,r4
        lvx     vr17,0,r3
        VPERM(vr16,vr16,vr16,byteswap)
        VPMSUMW(vr16,vr16,vr17)
        vxor    vr0,vr0,vr16
        bdz     .Ll5

        lvx     vr16,off16,r4
        lvx     vr17,off16,r3
        VPERM(vr16,vr16,vr16,byteswap)
        VPMSUMW(vr16,vr16,vr17)
        vxor    vr0,vr0,vr16
        bdz     .Ll5

        lvx     vr16,off32,r4
        lvx     vr17,off32,r3
        VPERM(vr16,vr16,vr16,byteswap)
        VPMSUMW(vr16,vr16,vr17)
        vxor    vr0,vr0,vr16
        bdz     .Ll5

        lvx     vr16,off48,r4
        lvx     vr17,off48,r3
        VPERM(vr16,vr16,vr16,byteswap)
        VPMSUMW(vr16,vr16,vr17)
        vxor    vr0,vr0,vr16
        bdz     .Ll5

        lvx     vr16,off64,r4
        lvx     vr17,off64,r3
        VPERM(vr16,vr16,vr16,byteswap)
        VPMSUMW(vr16,vr16,vr17)
        vxor    vr0,vr0,vr16
        bdz     .Ll5

        lvx     vr16,off80,r4
        lvx     vr17,off80,r3
        VPERM(vr16,vr16,vr16,byteswap)
        VPMSUMW(vr16,vr16,vr17)
        vxor    vr0,vr0,vr16
        bdz     .Ll5

        lvx     vr16,off96,r4
        lvx     vr17,off96,r3
        VPERM(vr16,vr16,vr16,byteswap)
        VPMSUMW(vr16,vr16,vr17)
        vxor    vr0,vr0,vr16

        /* Now xor all the parallel chunks together */
.Ll5:   vxor  vr0,vr0,vr1
        vxor    vr2,vr2,vr3
        vxor    vr4,vr4,vr5
        vxor    vr6,vr6,vr7

        vxor    vr0,vr0,vr2
        vxor    vr4,vr4,vr6

        vxor    vr0,vr0,vr4

.Lbarrett_reduction:
        /* Barrett constants */
#if defined(AIXPPC)
        laddr   r3,T.barrett_constants(r2)
#elif defined(LINUXPPC64)
        addis   r3,r2,.barrett_constants@toc@ha
        addi    r3,r3,.barrett_constants@toc@l
#else
        lis     r3,.barrett_constants@ha      
        addi    r3,r3,.barrett_constants@l
#endif

        lvx     const1,0,r3
        lvx     const2,off16,r3

        vsldoi  vr1,vr0,vr0,8
        vxor    vr0,vr0,vr1         /* xor two 64 bit results together */

#ifdef REFLECT
        /* shift left one bit */
        vspltisb vr1,1
        vsl     vr0,vr0,vr1
#endif

        vand    vr0,vr0,mask_64bit

#ifndef REFLECT
        /*
         * Now for the Barrett reduction algorithm. The idea is to calculate q,
         * the multiple of our polynomial that we need to subtract. By
         * doing the computation 2x bits higher (ie 64 bits) and shifting the
         * result back down 2x bits, we round down to the nearest multiple.
         */
        VPMSUMD(vr1,vr0,const1)         /* ma */
        vsldoi  vr1,zeroes,vr1,8        /* q = floor(ma/(2^64)) */
        VPMSUMD(vr1,vr1,const2)         /* qn */
        vxor    vr0,vr0,vr1             /* a - qn, subtraction is xor in GF(2) */

        /*
         * Get the result into r3. We need to shift it left 8 bytes:
         * vr0 [ 0 1 2 X ]
         * vr0 [ 0 X 2 3 ]
         */
        vsldoi  vr0,vr0,zeroes,8        /* shift result into top 64 bits */
#else
        /*
         * The reflected version of Barrett reduction. Instead of bit
         * reflecting our data (which is expensive to do), we bit reflect our
         * constants and our algorithm, which means the intermediate data in
         * our vector registers goes from 0-63 instead of 63-0. We can reflect
         * the algorithm because we don't carry in mod 2 arithmetic.
         */
        vand    vr1,vr0,mask_32bit      /* bottom 32 bits of a */
        VPMSUMD(vr1,vr1,const1)         /* ma */
        vand    vr1,vr1,mask_32bit      /* bottom 32bits of ma */
        VPMSUMD(vr1,vr1,const2)         /* qn */
        vxor    vr0,vr0,vr1             /* a - qn, subtraction is xor in GF(2) */

        /*
         * Since we are bit reflected, the result (ie the low 32 bits) is in
         * the high 32 bits. We just need to shift it left 4 bytes
         * vr0 [ 0 1 X 3 ]
         * vr0 [ 0 X 2 3 ]
         */
        vsldoi  vr0,vr0,zeroes,4        /* shift result into top 64 bits of */
#endif

        /* Get it into r3 */
        MFVSRD(r3, vs32)

.Lout:
        subi    r6,r1,64+10*16
        subi    r7,r1,64+2*16

        lvx     vr20,0,r6
        lvx     vr21,off16,r6
        lvx     vr22,off32,r6
        lvx     vr23,off48,r6
        lvx     vr24,off64,r6
        lvx     vr25,off80,r6
        lvx     vr26,off96,r6
        lvx     vr27,off112,r6
        lvx     vr28,0,r7
        lvx     vr29,off16,r7

        laddr   r31,-8(r1)
        laddr   r30,-16(r1)
        laddr   r29,-24(r1)
        laddr   r28,-32(r1)
        laddr   r27,-40(r1)
        laddr   r26,-48(r1)
        laddr   r25,-56(r1)

        blr

.Lfirst_warm_up_done:
        lvx     const1,0,r3
        addi    r3,r3,16

        VPMSUMD(vr8,vr16,const1)
        VPMSUMD(vr9,vr17,const1)
        VPMSUMD(vr10,vr18,const1)
        VPMSUMD(vr11,vr19,const1)
        VPMSUMD(vr12,vr20,const1)
        VPMSUMD(vr13,vr21,const1)
        VPMSUMD(vr14,vr22,const1)
        VPMSUMD(vr15,vr23,const1)

        b       .Lsecond_cool_down

.Lshort:
        cmpi    cr0,CmpAddr,r5,0
        beq     .Lzero

#if defined(AIXPPC)
        laddr   r3,T.short_constants(r2)
#elif defined(LINUXPPC64)
        addis   r3,r2,.short_constants@toc@ha
        addi    r3,r3,.short_constants@toc@l
#else
        lis     r3,.short_constants@ha      
        addi    r3,r3,.short_constants@l
#endif

        /* Calculate where in the constant table we need to start */
        subfic  r6,r5,256
        add     r3,r3,r6

        /* How many 16 byte chunks? */
        ShiftRI r7,r5,4
        mtctr   r7

        vxor    vr19,vr19,vr19
        vxor    vr20,vr20,vr20

        lvx     vr0,0,r4
        lvx     vr16,0,r3
        VPERM(vr0,vr0,vr16,byteswap)
        vxor    vr0,vr0,vr8            /* xor in initial value */
        VPMSUMW(vr0,vr0,vr16)
        bdz     .Lv0

        lvx     vr1,off16,r4
        lvx     vr17,off16,r3
        VPERM(vr1,vr1,vr17,byteswap)
        VPMSUMW(vr1,vr1,vr17)
        bdz     .Lv1

        lvx     vr2,off32,r4
        lvx     vr16,off32,r3
        VPERM(vr2,vr2,vr16,byteswap)
        VPMSUMW(vr2,vr2,vr16)
        bdz     .Lv2

        lvx     vr3,off48,r4
        lvx     vr17,off48,r3
        VPERM(vr3,vr3,vr17,byteswap)
        VPMSUMW(vr3,vr3,vr17)
        bdz     .Lv3

        lvx     vr4,off64,r4
        lvx     vr16,off64,r3
        VPERM(vr4,vr4,vr16,byteswap)
        VPMSUMW(vr4,vr4,vr16)
        bdz     .Lv4

        lvx     vr5,off80,r4
        lvx     vr17,off80,r3
        VPERM(vr5,vr5,vr17,byteswap)
        VPMSUMW(vr5,vr5,vr17)
        bdz     .Lv5

        lvx     vr6,off96,r4
        lvx     vr16,off96,r3
        VPERM(vr6,vr6,vr16,byteswap)
        VPMSUMW(vr6,vr6,vr16)
        bdz     .Lv6

        lvx     vr7,off112,r4
        lvx     vr17,off112,r3
        VPERM(vr7,vr7,vr17,byteswap)
        VPMSUMW(vr7,vr7,vr17)
        bdz     .Lv7

        addi    r3,r3,128
        addi    r4,r4,128

        lvx     vr8,0,r4
        lvx     vr16,0,r3
        VPERM(vr8,vr8,vr16,byteswap)
        VPMSUMW(vr8,vr8,vr16)
        bdz     .Lv8

        lvx     vr9,off16,r4
        lvx     vr17,off16,r3
        VPERM(vr9,vr9,vr17,byteswap)
        VPMSUMW(vr9,vr9,vr17)
        bdz     .Lv9

        lvx     vr10,off32,r4
        lvx     vr16,off32,r3
        VPERM(vr10,vr10,vr16,byteswap)
        VPMSUMW(vr10,vr10,vr16)
        bdz     .Lv10

        lvx     vr11,off48,r4
        lvx     vr17,off48,r3
        VPERM(vr11,vr11,vr17,byteswap)
        VPMSUMW(vr11,vr11,vr17)
        bdz     .Lv11

        lvx     vr12,off64,r4
        lvx     vr16,off64,r3
        VPERM(vr12,vr12,vr16,byteswap)
        VPMSUMW(vr12,vr12,vr16)
        bdz     .Lv12

        lvx     vr13,off80,r4
        lvx     vr17,off80,r3
        VPERM(vr13,vr13,vr17,byteswap)
        VPMSUMW(vr13,vr13,vr17)
        bdz     .Lv13

        lvx     vr14,off96,r4
        lvx     vr16,off96,r3
        VPERM(vr14,vr14,vr16,byteswap)
        VPMSUMW(vr14,vr14,vr16)
        bdz     .Lv14

        lvx     vr15,off112,r4
        lvx     vr17,off112,r3
        VPERM(vr15,vr15,vr17,byteswap)
        VPMSUMW(vr15,vr15,vr17)

.Lv15:  vxor    vr19,vr19,vr15
.Lv14:  vxor    vr20,vr20,vr14
.Lv13:  vxor    vr19,vr19,vr13
.Lv12:  vxor    vr20,vr20,vr12
.Lv11:  vxor    vr19,vr19,vr11
.Lv10:  vxor    vr20,vr20,vr10
.Lv9:   vxor    vr19,vr19,vr9
.Lv8:   vxor    vr20,vr20,vr8
.Lv7:   vxor    vr19,vr19,vr7
.Lv6:   vxor    vr20,vr20,vr6
.Lv5:   vxor    vr19,vr19,vr5
.Lv4:   vxor    vr20,vr20,vr4
.Lv3:   vxor    vr19,vr19,vr3
.Lv2:   vxor    vr20,vr20,vr2
.Lv1:   vxor    vr19,vr19,vr1
.Lv0:   vxor    vr20,vr20,vr0

        vxor    vr0,vr19,vr20

        b       .Lbarrett_reduction

.Lzero:
        mr      r3,r10
        b       .Lout

endproc.__crc32_vpmsum:
